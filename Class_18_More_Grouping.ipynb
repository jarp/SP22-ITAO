{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using apply() and groupby() to create your own groups\n",
    "\n",
    "Here we will show a simple use of apply() method and groupby() method that can be very useful. Note, that this *far more simple* use of the apply() method than the one shown in the advanced topics in Lecture 18 notebook. \n",
    "\n",
    "Let us say you want to group the 'AIRLINE' but not necessarily the same airline but according to the their alliance. How can you achieve this? THe following are the groups of airlines that are in each of the alliances\n",
    "\n",
    "Star Alliance:\n",
    "* UA - United Airlines\n",
    "* OO - Skywest Airlines\n",
    "\n",
    "Oneworld Alliance:\n",
    "* AA - American Airlines\n",
    "* US - US Airlines\n",
    "* MQ - American Eagle Airlines Inc. \n",
    "\n",
    "SkyTeam Alliance:\n",
    "* DL - Delta Airlines\n",
    "* EV - Atlantic Sotheast Airlines\n",
    "* VX - Virgin America\n",
    "\n",
    "NoAlliance; Not in any alliance:\n",
    "\n",
    "* F9 - Forntier\n",
    "* B6 - Jetblue\n",
    "* NK - Spirit\n",
    "* WN - Southwest\n",
    "* HA - Hawaiian\n",
    "* AS - Alaskan \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('./data/flight_sample.csv')\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alliance(airline):\n",
    "    if airline in ['UA','OO']:\n",
    "        return 'Star'\n",
    "    elif airline in ['AA', 'US', 'MQ']:\n",
    "        return 'Oneworld'\n",
    "    elif airline in ['DL','EV', 'VX']:\n",
    "        return 'SkyTeam'\n",
    "    elif airline in ['AS', 'F9', 'B6', 'NK', 'WN', 'HA']:\n",
    "        return 'NoAlliance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "get_alliance('OO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING a new column called 'Alliance' and assigning the alliance based on the function\n",
    "flights['Alliance'] = flights['AIRLINE'].apply(get_alliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_by_alliance = flights.groupby(['Alliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flights_by_alliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = flights_by_alliance['DISTANCE','TAXI_IN'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means.loc['Star']['DISTANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour:  `sort_values()`, a method to sort rows based on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means.sort_values(['DISTANCE'], inplace=True, ascending=False)\n",
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity:\n",
    "\n",
    "1. Drop all rows in `college_scorecard_small` that has any missing values\n",
    "\n",
    "2. Add another column `sat_avg_level` to the `college_scorecard_small` DataFrame. It is assigned the following values based on the values in `sat_average`. **You need to write a function and use ``apply()`` method**\n",
    "   * Lower_sat\n",
    "       - sat_average <= 973 \n",
    "   * Below_avg_sat\n",
    "       - 973 < sat_average <= 1039\n",
    "   * Abv_avg_sat\n",
    "       - 1039 < sat_average <= 1120\n",
    "   * Higher_sat\n",
    "       - sat_average> 1120\n",
    "   \n",
    "3. Is there a relationship between `sat_avg_level` and `pell_grant_receipents`? How about relationship with `full_time_retention_rate_4_year`? \n",
    "   * Group by sat_avg_level and find the average for the rest of the two columns and make your interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_scorecard = pd.read_csv(\n",
    "    './data/college-scorecard-data-scrubbed.csv', \n",
    "    encoding='latin-1')\n",
    "\n",
    "# I'm extracting only three columns and creating a copy for this analysis. \n",
    "college_scorecard_small = college_scorecard[['sat_average', 'pell_grant_receipents','full_time_retention_rate_4_year']].copy()\n",
    "college_scorecard_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any data with missing data and make sure it stays that way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to categorize the sat_average values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a sat_avg_level column using the function above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by our new column\n",
    "\n",
    "# get the averages of 'pell_grant_receipents','full_time_retention_rate_4_year'\n",
    "# display the reuslts\n",
    "\n",
    "# how would you get the \n",
    "# * min and max of the sat averages \n",
    "# * meidan pell grant recipients \n",
    "# * average retention rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Tables: Two-dimensional GroupBy\n",
    "\n",
    "We have seen how the ``GroupBy`` abstraction lets us explore relationships within a dataset.\n",
    "A *pivot table* is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data.\n",
    "The pivot table takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index is the row grouping and columns is the column grouping, \n",
    "# the first parameter is the one that is aggregated\n",
    "\n",
    "flight_pvt = flights.pivot_table(\n",
    "        'DISTANCE',\n",
    "        index='DAY_OF_WEEK', \n",
    "        columns = 'Alliance'\n",
    "        )\n",
    "\n",
    "flight_pvt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the default behavior with `aggfunc` keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_pvt = flights.pivot_table('DISTANCE',index='DAY_OF_WEEK', \n",
    "                                 columns = 'Alliance', \n",
    "                                 aggfunc = np.sum)\n",
    "flight_pvt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can get the totals using `margins` keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_pvt = flights.pivot_table('DISTANCE',index='DAY_OF_WEEK', \n",
    "                                 columns = 'Alliance', \n",
    "                                 aggfunc = np.sum, \n",
    "                                 margins=True)\n",
    "flight_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get second day of the week for 'No Alliance'\n",
    "flight_pvt.loc[2]['NoAlliance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.groupby(['DAY_OF_WEEK','Alliance'])[['DISTANCE']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Birthrate Data\n",
    "\n",
    "As a more interesting example, let's take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC).\n",
    "This data can be found at https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv\n",
    "(this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, [this blog post](http://andrewgelman.com/2012/06/14/cool-ass-signal-processing-using-gaussian-processes/)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a column called `decade` in the births_df dataframe loaded below. \n",
    "    * Use the column called `year` to create the `decade`. For example, if you were born in 1969 it should say your decade is 1960\n",
    "2. Create a pivot table that counts number of `births` in each decade and also based on whether they were male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_df = pd.read_csv('./data/births.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1969//10)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'decade' column and use the formula (births_df['year'] // 10 ) * 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table that counts number of births in each decade and also based on whether they were male or female\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h3> CAUTION AHEAD </h3>\n",
    "<p> </p>\n",
    "<p> The topics discussed ahead are advanced and you need to absolutely make sure you understand everything discussed in the previous classes to move forward.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Topics: filter() and transform()\n",
    "\n",
    "These functions give a lot more flexibility on `DataFrameGroupBY` objects and they are discussed below. They are advanced topics, however, I **strongly encourage** you to read through them and you could use them for finding very interesting patterns in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for the Advanced Topics, you will need to run this before you use them further. \n",
    "college_loan_defaults = pd.read_csv(\n",
    "    './data/college-loan-default-rates.csv')\n",
    "\n",
    "college_scorecard = pd.read_csv(\n",
    "    './data/college-scorecard-data-scrubbed.csv', \n",
    "    encoding='latin-1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `filter()` Method\n",
    "\n",
    "You can use the `filter()` method to generate a new dataframe after filtering out groups that don't pass a given criteria. It allows you to answer questions like this: *what states in college scorecard have rows where the average SAT score (for the state) is above 1100?*\n",
    "\n",
    "To use this method, you must pass in a function that takes a single parameter, which is the group to evaluate. The function must return either `True`/`False` depending on whether or not the *rows of the group* should be kept or discarded in the new dataframe.\n",
    "\n",
    "So, with this in mind, let's define a `sat_filter` function so that groups with average SAT scores of less than 1100 are dropped from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges_by_state = college_scorecard.groupby(['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_filter(group):\n",
    "    if group['sat_average'].mean() >= 1150:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use it on to see which rows remain in the new dataframe after applying the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_scorecard[['state','city', 'sat_average']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't work because it's not grouped\n",
    "# college_scorecard[['state','city', 'sat_average']].filter(size_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to reduce the complexity here, I'm only going\n",
    "# to display the `sat_average`, `state`, and `city` fields \n",
    "filter_results = colleges_by_state[['institution_name','state','city', 'sat_average']].filter(size_filter)\n",
    "filter_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ***really*** important things to notice here:\n",
    "1. Unlike the **`aggregate`** method, the data returned here is not grouped by state as you probably expected it to be. The filter is used on a grouped dataframe, but it returns a new \"normal\" dataframe.\n",
    "2. Notice that we have a bunch of rows for Washington DC and Rhode Island, but nothing else. If we've done things correctly, this would mean that the colleges in those two states have average SAT scores of at least 1150. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `transform()` Method\n",
    "\n",
    "You use the **`transform()`** method to generate a new dataframe that modifies/transforms the values of the grouped dataframes columns.\n",
    "\n",
    "That probably just confused the heck out of you. So we will start with a practical example.\n",
    "\n",
    "Let's say that we wanted to center the data for the *`year_1_default_rate`* and *`year_2_default_rate`* columns of our **`college_loan_defaults_by_state`** grouped dataframe. \n",
    "\n",
    "Let's step through how we could do that with **`transform()`**.\n",
    "\n",
    "Just like with the **`filter()`** method, we have to create a function that we will pass to the **`transform`** method, but this time the function will evaluate each series (column) of each group, rather than the groups as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just extracting three columns for this analysis\n",
    "college_loan_defaults_subset = college_loan_defaults[['name', 'state', 'year_1_default_rate']]\n",
    "college_loan_defaults_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_loan_defaults_by_state = college_loan_defaults_subset.groupby('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the mean for each state\n",
    "college_loan_defaults_by_state['year_1_default_rate'].mean()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be called on each \n",
    "# series of each group in your DataFrameGroupBy object\n",
    "def center_default_rate(series):\n",
    "    return series - series.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also use the rename() method to apply some friendly column names.\n",
    "transformed_default_rates = college_loan_defaults_by_state.transform(\n",
    "    center_default_rate).rename(\n",
    "        columns={'year_1_default_rate': 'centered_year_1_default_rate'})\n",
    "\n",
    "transformed_default_rates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "Our `college_loan_defaults_by_state` dataframe included four columns: name, state, and year_1_default_rate.\n",
    "</p> \n",
    "<p>But here in the returned dataframe we only have `centered_year_1_default_rate`. The reason for this is that the other two columns were strings, and you can't calculate the mean of a series of strings.\n",
    "</p>\n",
    "<p>\n",
    "Because of this, Pandas just silently drops them from the new dataframe that is returned from the `tranform` method.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our centered rates in a new dataframe. Let's merge together the result of our **`transform`** method and our *`college_loan_defaults_subset`* dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to specify the indices as the \"join column\" or Pandas\n",
    "# will try to join the dataframes based on the shared 'year_1_default_rate' column.\n",
    "pd.merge(college_loan_defaults_subset, transformed_default_rates, \n",
    "         left_index=True, \n",
    "         right_index=True)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
