{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learing with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"./data/bp_classific.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot to understand the effect of various features\n",
    "\n",
    "You can specific the column you want to use for classification (in this case `high_bp`) as `hue` parameter to distinguish between one class to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(bp_data, hue='high_bp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Classfication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the input features and outcome variable\n",
    "\n",
    "bp_data_X = bp_data.drop('high_bp', axis = 1)\n",
    "bp_data_Y = bp_data['high_bp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data_Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split()`: Method to split the data into train and test\n",
    "\n",
    "We usually split the data into training set to learn a classifier and then a test set to validate how good our model is \n",
    "\n",
    "Important parameters to this method\n",
    "\n",
    "* **random_state**: Seed to used by randomizer to randomly split the data\n",
    "* **train_size**: Use float to specify what fraction to use for training. Usually 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bp_train_X, bp_test_X, bp_train_Y, bp_test_Y = train_test_split(\n",
    "    bp_data_X, bp_data_Y, \n",
    "    random_state=42, \n",
    "    train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bp_data_X), len(bp_train_X), len(bp_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many classifiers...\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(bp_train_X, bp_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_predict_Y = model.predict(bp_test_X)\n",
    "bp_predict_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklmetrics\n",
    "sklmetrics.accuracy_score(bp_test_Y, bp_predict_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = sklmetrics.confusion_matrix(bp_test_Y, bp_predict_Y, labels =[0,1])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False)\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the above case we can see that we have \n",
    "\n",
    "Good cases:\n",
    "* **True Negatives**: 10 cases when the true value was No High BP (high_bp = 0) and we predicted that there will be No High BP \n",
    "* **True Positives**: 14 cases when the true value was High BP (high_bp = 1) and we predicted that there will be High BP \n",
    "\n",
    "Bad cases:\n",
    "* **False Positives**: 4 cases when the true value was No High BP (high_bp = 0) and we predicted that there will be High BP (TYPE I ERROR)\n",
    "* **False Negatives**: 2 cases when the true value was High BP (high_bp = 1) and we predicted that there will be No High BP (TYPE II ERROR)\n",
    "\n",
    "Now, you can think about the consequence of making both these mistakes (related to Type I and Type II errors), how it might effect a doctor to use these methods in real-world "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "We will use the breast cancer data available in scikit-learn to predict if an biopsy image is cancerous or not. \n",
    "\n",
    "Follow these steps (steps 1 and 2 are done for you)\n",
    "1. Load the data \n",
    "2. Seperate X (input features) and Y (outcome)\n",
    "3. Split into training data and test data. Use 80% of data for training\n",
    "    * Verify if the data is appropriately split by checking the number of rows in each of the training and test data. \n",
    "4. Learn the GaussianNB classifier to predict cancer or malignant\n",
    "5. Predict using the test data\n",
    "6. Provide accuracy score as well as plot the confusion matrix\n",
    "    * Think about the consequence of False Positives and False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Datasets available in the package\n",
    "\n",
    "The scikit-learn package has datasets available in the library which makes it is easy to practice and learn without needing to load it from the data through a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Breast cancer dataset\n",
    "\n",
    "We are going to work with a sample dataset which is already available in the package and can be loaded by just calling `load_breast_cancer()` method. It is [available here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic%29). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "cancer = sklearn.datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(cancer.data, columns=[cancer.feature_names])\n",
    "cancer_df['target'] = pd.Series(cancer.target)\n",
    "cancer_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Seperate X (input features) and Y (outcome)\n",
    "cancer_X = cancer.data\n",
    "cancer_Y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test data using train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the data is split up as intended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lean the data using the GaussianNB model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Unsupervised Learning\n",
    "\n",
    "The example below is borrowed from your textbook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing the digits data\n",
    "\n",
    "We'll use Scikit-Learn's data access interface and take a look at this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE HOW THE FOLLOWING CODE WORKS BUT CONCENTRATE ON THE HOW THE DATA LOOKS\n",
    "# Convert the data into a DataFrame\n",
    "\n",
    "digits_df = pd.DataFrame(digits.data)\n",
    "digits_df['Target'] = pd.Series(digits.target)\n",
    "digits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images data is a three-dimensional array: 1,797 samples each consisting of an 8 × 8 grid of pixels.\n",
    "Let's visualize the first hundred of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT WORRY HOW THE IMAGES ARE LOADED\n",
    "## CONCENTRATE ON THE IMAGES THAT ARE PRODUCED AS OUTPUT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into X (input characteristics) and Y (outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_X = digits.data\n",
    "digit_Y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_Y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data.\n",
    "Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!\n",
    "\n",
    "Here we will use principal component analysis (PCA), which is a fast linear dimensionality reduction technique.\n",
    "We will ask the model to return two components—that is, a two-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  \n",
    "model = PCA(n_components=2)            \n",
    "model.fit(digit_X)                      \n",
    "X_2D = model.transform(digit_X)\n",
    "X_2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df['PCA1'] = X_2D[:, 0]\n",
    "digits_df['PCA2'] = X_2D[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='Target', data=digits_df, fit_reg=False, height=10,\n",
    "          palette = sns.color_palette(\"Set1\", n_colors=10, desat=.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning: Iris clustering\n",
    "\n",
    "Let's next look at applying clustering to the digits data.\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels.\n",
    "Here we will use a powerful clustering method called a Gaussian mixture model (GMM). \n",
    "A GMM attempts to model the data as a collection of Gaussian blobs. \n",
    "\n",
    "We can fit the Gaussian mixture model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model class\n",
    "from sklearn.mixture import GaussianMixture      \n",
    "# 2. Instantiate the model with hyperparameters. \n",
    "# We are building 10 clusters (n_components) because we believe they \n",
    "# may be 10 clusters, one for each number\n",
    "model = GaussianMixture(n_components=10,\n",
    "            covariance_type='full')  \n",
    " # 3. Fit to data. Notice y is not specified!\n",
    "model.fit(digit_X)  \n",
    "# 4. Determine cluster labels\n",
    "y_gmm = model.predict(digit_X)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the clusters and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again ignore the technical details\n",
    "\n",
    "digits_df['cluster'] = y_gmm\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", data=digits_df, hue='Target',\n",
    "           col='cluster', fit_reg=False, col_wrap=3,\n",
    "          palette = sns.color_palette(\"Set1\", n_colors=10, desat=.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Important Note**\n",
    "\n",
    "The cluster numbers may not correspond to actual digit they represent. From the image, see if you can find which cluster number corresponds to which number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
