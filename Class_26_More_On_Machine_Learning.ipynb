{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game of Thrones Dataset\n",
    "\n",
    "In this section, we will use the dataset based on popular book series (and HBO TV series) from George RR Martin, Game of Thrones. The dataset was made available through [Kaggle](https://www.kaggle.com/mylesoneill/game-of-thrones/data) which has information on the character deaths. The dataset was cleaned and we will be working with a sample dataset for this analysis. \n",
    "\n",
    "Game of Thrones is known for abruptly ending its characters. We will use machine learning methods to predict if a character will be alive or dead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "got_data = pd.read_csv(\"./data/GoT_Character_Deaths.csv\")\n",
    "print(got_data.shape)\n",
    "got_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data also includes the 'Name' of the person and the 'Allegiances'. We will remove 'Name' as the name itself is not indicative if the character will alive or dead. We will also remove 'Allegiances' for now as we do not know how to handle categorical datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data.drop(['Name', 'Allegiances'], axis = 1, inplace=True)\n",
    "got_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classfication using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the input features and outcome variable\n",
    "\n",
    "got_data_X = got_data.drop('dead',1)\n",
    "got_data_Y = got_data['dead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split()`: Method to split the data into train and test\n",
    "\n",
    "We usually split the data into training set to learn a classifier and then a test set to validate how good our model is \n",
    "\n",
    "Important parameters to this method\n",
    "\n",
    "* **random_state**: Seed to used by randomizer to randomly split the data\n",
    "* **train_size**: Use float to specify what fraction to use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "got_train_X, got_test_X, got_train_Y, got_test_Y = train_test_split(got_data_X, got_data_Y, \n",
    "                                                                    random_state=42, \n",
    "                                                                    train_size = 0.7\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(got_data_X), len(got_train_X), len(got_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a classifier: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_regression_model = LogisticRegression()\n",
    "\n",
    "log_regression_model.fit(got_train_X, got_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_predict_Y = log_regression_model.predict(got_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklmetrics\n",
    "\n",
    "sklmetrics.accuracy_score(got_test_Y, got_predict_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = sklmetrics.confusion_matrix(got_test_Y, got_predict_Y, labels =[0,1])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(conf_mat, square=True, annot=True, fmt='2g', cbar = False, xticklabels = ['Alive','Dead'], yticklabels = ['Alive','Dead'], ax=ax)\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "ax.set_ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot coefficients as feature importance\n",
    "# INPUT: Used for Logistic Regression Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most Coefficients\n",
    "def plot_feature_importance_coeff(model, Xnames, cls_nm = None):\n",
    "\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.coef_.ravel())), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importance_coeff(log_regression_model, got_data_X.columns, cls_nm=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dec_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "dec_tree_model.fit(got_train_X, got_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_predict_Y = dec_tree_model.predict(got_test_X)\n",
    "\n",
    "print(sklmetrics.accuracy_score(got_test_Y, got_predict_Y))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(got_test_Y, got_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False, \n",
    "            xticklabels = ['Alive','Dead'], \n",
    "            yticklabels = ['Alive','Dead'],\n",
    "            fmt='2g')\n",
    "\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot feature importance for trees\n",
    "# INPUT: Used for Tree based Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most features\n",
    "\n",
    "def plot_feature_importance(model, Xnames, cls_nm = None):\n",
    "\n",
    "    # Measuring important features\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(dec_tree_model, got_data_X.columns, cls_nm='Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Campaign Dataset\n",
    "\n",
    "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#), that provides information on the phone campaign run by the bank to see if a customer can be converted to have term deposit at their bank. We will only be using a sample from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_data['success'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Class Imbalance problem with `class_weight='balanced'`\n",
    "\n",
    "Notice below that in the above bank campaign dataset there are more failures than successes. In these case it is important to let the classifier know that it needs to handle the class imbalance problem. There are many ways to handle the class imbalance problem including oversampling and under sampling, changing the cost of making False Negatives and False Positives. \n",
    "\n",
    "We can do that by creating a classifier with the parameter `class_weight='balanced'`. In that way, the classifier handles the class imbalance problem by choosing the appropriate cost of making False Negatives and False Positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.drop(['marital','education','contact'], axis=1, inplace=True)\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_X = bank_data.drop('success', axis = 1)\n",
    "bank_data_Y = bank_data['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_train_X, bank_test_X, bank_train_Y, bank_test_Y = train_test_split(\n",
    "                    bank_data_X, \n",
    "                    bank_data_Y, \n",
    "                    random_state = 42, \n",
    "                    train_size = 0.7\n",
    "                    )\n",
    "\n",
    "bank_logistic = LogisticRegression()\n",
    "bank_logistic.fit(bank_train_X, bank_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_predict_Y = bank_logistic.predict(bank_test_X)\n",
    "print(\"The accuracy is {0}\".format(sklmetrics.accuracy_score(bank_test_Y, bank_predict_Y)))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_test_Y, bank_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, fmt=\"2g\", cbar = False, xticklabels = ['Failure','Success'], \n",
    "            yticklabels = ['Failure','Success'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_logistic_balanced = LogisticRegression(class_weight='balanced')\n",
    "bank_logistic_balanced.fit(bank_train_X, bank_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_predict_Y = bank_logistic_balanced.predict(bank_test_X)\n",
    "print(\"The accuracy is {0}\".format(sklmetrics.accuracy_score(bank_test_Y, bank_predict_Y)))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_test_Y, bank_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, fmt=\"2g\", cbar = False, xticklabels = ['Failure','Success'], \n",
    "            yticklabels = ['Failure','Success'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Input Variables using `pd.get_dummies()` method\n",
    "\n",
    "In the previous class, we have deleted the categorical input variables, but you don't have to always delete them. You can convert the cateogrical input variables using `get_dummies()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h5>Remove the one category for each categorical variable</h5>\n",
    "<p>\n",
    "As you will see the `get_dummies` method introducing a column for each value of the cateogorical varible. It is **very important** to remove one column for each categorical variable for the models to appropriately work. \n",
    "</p>\n",
    "<p>\n",
    "For example, in for the bank data, for the categorical variable marital status, may be we can remove say 'marital_divorced', and for education may be we can remove 'education_unknown' and for contact we can remove 'contact_unknown'. \n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_data['marital'].unique())\n",
    "print(bank_data['education'].unique())\n",
    "print(bank_data['contact'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies = pd.get_dummies(bank_data)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies.drop(['marital_divorced','education_unknown','contact_unknown'], axis = 1, inplace=True)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bank_data_with_dummies_auto = pd.get_dummies(bank_data, drop_first=True)\n",
    "bank_data_with_dummies_auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the Classification models\n",
    "\n",
    "Every classification model has set of parameters that need to be set before the model can be learned. In the folllowing we explain a couple of important parameters for Logistic Regression and Decision Tree Classification\n",
    "\n",
    "* **Logistic Regression**: When you are running logistic regression, which is very similar to multiple linear regression, having a lot of input variables can lead to overfit in the data. Hence, hence there number of ways to reduce the number of input variables used in the logistic regression. These methods are called **variable selection** methods. One classic machine learning method to do the variable selection with the data is through **regularization**. You can think of regularization as selecting only a subset of input features that can be used in the logistic regression. Rather than doing manually, regularization is a statistically rigorous way of doing the **variable selection**. There are two parameters with respect to regularization in LogisticRegression classifier. The main idea of these parameters is to **penalize models the use more of input variables** in the logistic regression\n",
    "    * **penalty**: This is a kind of penalty you wish to apply. The most common ones are ['l1','l2']. \n",
    "    * **C**: The weight that needs to be given for regularization. The more the value of C, the more preference for using more input variables. The smaller the value of C, the stronger the regularization and hence less input variables is preferred. The most common values are from [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "    \n",
    "    \n",
    "* **Decision Tree**: Recollect decision tree is building a tree with increasing depth. The two important parameters to set are \n",
    "    * **max_depth**: How deep should the decision tree be built. Usually, depends on the data and most common values are [3,4,5]\n",
    "    * **max_features**: How many features should be considered when splitting a decision tree node. The most common values are ['auto','log2', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "bank_dummies_X = bank_data_with_dummies.drop(['success'], axis = 1)\n",
    "bank_dummies_Y = bank_data_with_dummies['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into training and test data.  Use 70% of the data for training.\n",
    "bank_dum_train_X, bank_dum_test_X, bank_dum_train_Y, bank_dum_test_Y = train_test_split(bank_dummies_X, bank_dummies_Y, \n",
    "                                                                                       random_state=37,\n",
    "                                                                                       train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: Learn the classifier with various parameters on the training data for Logistic Regression.  \n",
    "# Try these combinations\n",
    "\n",
    "# penalty = 'l1', C = 0.1\n",
    "# penalty = 'l1', C = 1\n",
    "# penalty = 'l1', C = 10\n",
    "# penalty = 'l2', C = 0.1\n",
    "# penalty = 'l2', C = 0.1\n",
    "# penalty = 'l2', C = 10\n",
    "\n",
    "log_reg_model_parameter = LogisticRegression(class_weight='balanced', \n",
    "                                             penalty='l1', C= 100\n",
    "                                            )\n",
    "log_reg_model_parameter.fit(bank_dum_train_X, bank_dum_train_Y)\n",
    "\n",
    "# Step 4: Use the test data to predict the Y and then print the accuracy_score for Logistic Regression \n",
    "logistic_bank_predict_Y = log_reg_model_parameter.predict(bank_dum_test_X)\n",
    "#Get an accuracy\n",
    "sklmetrics.accuracy_score(bank_dum_test_Y, logistic_bank_predict_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Learn the classifier with various parameters on the training data for Decision Tree. \n",
    "# Try these combinations\n",
    "\n",
    "# max_depth = 2, max_features = 'auto'\n",
    "# max_depth = 3, max_features = 'auto'\n",
    "# max_depth = 2, max_features = 'log2'\n",
    "# max_depth = 3, max_features = 'log2'\n",
    "# max_depth = 2, max_features = None\n",
    "# max_depth = 3, max_features = None\n",
    "\n",
    "\n",
    "dec_tree_model_parameter = DecisionTreeClassifier(class_weight='balanced', \n",
    "                                                  max_depth = 3, max_features=None\n",
    "                                                 )\n",
    "dec_tree_model_parameter.fit(bank_dum_train_X, bank_dum_train_Y)\n",
    "\n",
    "# Step 4: Use the test data to predict the Y and then print the accuracy_score for Decision Tree\n",
    "decision_tree_bank_predict_Y = dec_tree_model_parameter.predict(bank_dum_test_X)\n",
    "sklmetrics.accuracy_score(bank_dum_test_Y, decision_tree_bank_predict_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h5>DO NOT SELECT THE PARAMETERS BASED ON TEST DATA </h5>\n",
    "<p> We have to rather use cross validation techniques discussed below</p>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation to Select the Parameters of a Model using `GridSearchCV()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, selecting the model parameters has to be done in a cross validation manner. Refer to lecture notes (pdf, pptx) in Sakai for more understanding on cross validation. \n",
    "\n",
    "Scikit-Learn provides a wide variety of cross validation techniques, but the most common way is using `GridSearchCV()` method.\n",
    "\n",
    "**GridSearchCV()** is the process of searching through every possible combination of parameters and selecting the best parameter combination. For example\n",
    "\n",
    "```python\n",
    "params = {'max_depth':[2,3],\n",
    "          'max_features':['auto','log2',None]}\n",
    "```\n",
    "\n",
    "Then you have 6 different combination of parameters listed below. Hence, its called grid search. \n",
    "* max_depth = 2, max_features = 'auto'\n",
    "* max_depth = 3, max_features = 'auto'\n",
    "* max_depth = 2, max_features = 'log2'\n",
    "* max_depth = 3, max_features = 'log2'\n",
    "* max_depth = 2, max_features = None\n",
    "* max_depth = 3, max_features = None\n",
    "\n",
    "** Finally you can select the best model, based on a metric (usually accuracy).** \n",
    "\n",
    "\n",
    "`GridSearchCV()` is very useful method that automates this process of selecting the model with the best set of parameters. The method has four main parameters\n",
    "\n",
    "* **estimator**: The classifier you want to learn the parameters, LogisticRegression, DecisionTreeClassifier, etc. \n",
    "* **param_grid**: Dictionary (dict) of parameters and their values to be searched over. \n",
    "* **cv**: How many folds of classification you want to use. Usually 3 for smaller data and 10 for large data. \n",
    "* **n_jobs**: Usually you specify this as 1. You can parallelize the process of this search by specifying a value more than 1. **Do not have the n_jobs set to more than 3**, for the first time users. Especially, on a laptop or lab machine or on Vocareum, you will end up stalling the machine and it has to be rebooted. For more experienced students, in the class, check the number of processing cores of the machine before you increase the number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree based classification\n",
    "\n",
    "\n",
    "# The model you want to set the parameters for\n",
    "model = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "# The parameters to search over for the model\n",
    "params = {'max_depth':[2,3,4],\n",
    "          'max_features':['auto','log2',None]}\n",
    "\n",
    "\n",
    "# Prepare the GridSearch for cross validation\n",
    "grid_search_dec_tree = GridSearchCV(model, # Note the model is DecisionTreeClassifier as stated above\n",
    "                                    param_grid=params, # The parameters to search over. \n",
    "                                    cv=3, # How many hold out sets to use\n",
    "                                    n_jobs = 1 # Number of parallel processes to run. \n",
    "                                    )\n",
    "\n",
    "# Do the cross validation on the training data \n",
    "grid_search_dec_tree.fit(bank_dum_train_X, bank_dum_train_Y)\n",
    "\n",
    "# Select the best model\n",
    "best_dec_tree_cv = grid_search_dec_tree.best_estimator_\n",
    "\n",
    "# Print the best parameter combination \n",
    "print(grid_search_dec_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "1. Usually for large datasets, the above `GridSearchCV` method takes a lot of time. You might have to make sure, you run with limited set of parameters, before you increase the number of possible values for the parameters. \n",
    "2. Everytime you run the GridSearchCV, you might find a different combination of parameters to be the best one. This is another issue with **consistency** of machine learning algorithms. Addressing this is out of the scope for this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally test the performance of the best model on the test data\n",
    "\n",
    "bank_dum_pred_Y = best_dec_tree_cv.predict(bank_dum_test_X)\n",
    "\n",
    "#Print the accuracy \n",
    "print(sklmetrics.accuracy_score(bank_dum_test_Y, bank_dum_pred_Y))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_dum_test_Y, bank_dum_pred_Y)\n",
    "print(conf_mat)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(conf_mat, fmt='d',square=True, annot=True, cbar = False, \n",
    "            xticklabels = ['Failure','Success'], \n",
    "            yticklabels = ['Failure','Success']\n",
    "           )\n",
    "\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.ylim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot feature importance for trees\n",
    "# INPUT: Used for Tree based Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most features\n",
    "\n",
    "def plot_feature_importance(model, Xnames, cls_nm = None):\n",
    "\n",
    "    # Measuring important features\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    imp_features = imp_features.iloc[10:]\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(best_dec_tree_cv, bank_dum_train_X.columns, cls_nm='Best CV Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the performance of classification\n",
    "\n",
    "Until now, we have been always using `accuracy_score` to verif the performance of the classification on the test data. However, in reality this is not usually the most optimal metric. There are other important metrics to use in real-world, shown below. Again, the discussion on these metrics is out of the scope for this course. \n",
    "\n",
    "1. Precision, Recall, F1 Score\n",
    "2. Area Under the Curve (AUC) of Receiver Operating Characteristic (ROC) curve\n",
    "3. Area under Precision-Recall Curve\n",
    "4. Specificity and Sensitivity\n",
    "5. Positive predictive value\n",
    "6. ... many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Many more classification techniques\n",
    "\n",
    "We discussed two basic models in this section. However, there are many other classifiers you can use to improve your classification accuracy. Below, I have provided three main classification methods (by no means they are exhaustive)\n",
    "\n",
    "* MLPClassifier: Multi Layer Perceptron model. This is a very basic model that is a primer to deep learning neural networks. \n",
    "    ```python\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    ```\n",
    "* GradientBoostClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostClassifier\n",
    "    ```\n",
    "* RandomForestClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
