{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Working with Strings in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_deaths = pd.read_csv(\"./data/GoT_Character_Deaths.csv\")\n",
    "got_deaths.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jonathan Arp\" \n",
    "name.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_deaths['ALL_CAPS'] = got_deaths['Allegiances'].str.upper()\n",
    "got_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_deaths['also allegiances, but shouted'] = got_deaths['Allegiances'].str.upper() + \"!!!\"\n",
    "got_deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scrapping is very large concept and involves a deep understanding of how websites are created and managed. You will also need to know some fundamentals of HTML. In this section we will do a very basic foundations of extracting the data from the websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.read_html()`\n",
    "\n",
    "Using the pandas package, you can read the tables that are created on the websites. It reads all the tables that are available on the webpage. \n",
    "\n",
    "The following example extracts the NBA 2017 draft data set from the [Sports Reference](https://www.basketball-reference.com/draft/NBA_2017.html) website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_data_list = pd.read_html(\"https://www.basketball-reference.com/draft/NBA_2017.html\") \n",
    "print(type(nba_data_list))\n",
    "print(len(nba_data_list))\n",
    "type(nba_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that after `read_html()` returns a list. There can be multiple tables in a given webpage. The `read_html()` method returns list of tables. In this webpage there is only one table. So you can access the table with the 0th indexed element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df = nba_data_list[0]\n",
    "nba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the web pages is not always clean. In this case you might have observed the column names are all multilevel indexes. You can change the column names as indicated on the website by renaming the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.columns = ['Rk', 'Pk', 'Tm','Player','College', 'Yrs','G', 'MP', 'PTS','TRB','AST','FG%', \n",
    "                    '3P%', 'FT%', 'MP', 'PTS', 'TRB', 'AST', 'WS', 'WS/48', 'BPM', 'VORP']\n",
    "\n",
    "nba_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data\n",
    "\n",
    "Data downloaded from the webpages, most certainly requires to be cleaned. The following is a simple example of deleting unnnecessary data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the internet data is **messy**. For example, if you actually see the rows from 28:34, you will see that index 30, 31 had data that is not required. Look at the [website](https://www.basketball-reference.com/draft/NBA_2017.html) the table has a break, so the the DataFrame has unnecessary information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.loc[28:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop those two rows with those indices and you are saying inplace=True, to make sure you are not creating a copy. \n",
    "nba_df.drop([30,31], axis=0, inplace=True)\n",
    "nba_df.loc[28:34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on cleaning up data \n",
    "\n",
    "We are goin gto download the top 250 movies from [IMDB](http://www.imdb.com/chart/top?ref_=nv_wl_img_3) list \n",
    "\n",
    "We need to clean the data and remove unnecessary rows and columns like before. But there's more we want to do. \n",
    "\n",
    "Notice that the Title acutally has the date of the movie in it. That's not helpful. Wouldn't it be great to have a column that had the date. That would be very useful for our data analysis goals\n",
    "\n",
    "Like, asking which movie released in 2014 has highest IMDb rating. Or which year had the highest average rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_html(\"https://www.imdb.com/chart/top?ref_=nv_wl_img_3\")[0]\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns\n",
    "let's drop the columns that have no useful data. First he pass in a `list` of columns. Remember that the default is to delete rows, so we add `axis=1` to tell pandas we are dropping collumns. Lastly we want the changes to remain so we add `inplace=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop([\"Unnamed: 0\", \"Unnamed: 4\", \"Your Rating\"], axis=1, inplace=True)\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the str function, let's grab the year and put it in a column called 'year'\n",
    "movie_df['year'] = movie_df['Rank & Title'].str[-5:-1]\n",
    "movie_df['Rank & Title'] = movie_df['Rank & Title'].str[0:-6]\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['ranking'] = movie_df['Rank & Title'].str.extract('(\\d{1,4}).\\ ')\n",
    "movie_df['title'] = movie_df['Rank & Title'].str.extract('\\d{1,4}.\\ (.*)')\n",
    "movie_df.drop(['Rank & Title'], axis=1, inplace=True)\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_14_df = movie_df[movie_df['year'] == '2014']\n",
    "df_sorted = movie_14_df.sort_values('IMDb Rating', ascending=False)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_14_df[ movie_14_df['IMDb Rating'] == movie_14_df['IMDb Rating'].max() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the highest average rankings for each year\n",
    "movies_by_year = movie_df.groupby('year').mean()\n",
    "# Highest ranked year\n",
    "movies_by_year.loc[ movies_by_year['IMDb Rating'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_by_year\n",
    "# movies_by_year['IMDb Rating'].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages for webscrapping \n",
    "\n",
    "* urllib\n",
    "* requests\n",
    "* **BeautifulSoup**\n",
    "* mechanize\n",
    "\n",
    "This will require some fundamentals on HTML, the language used to display the webpages on the browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://simple.wikipedia.org/wiki/List_of_U.S._state_capitals\")\n",
    "page = req.text\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup = BeautifulSoup(page, 'html.parser')\n",
    "page_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the actual webpage and its contents. \n",
    "\n",
    "**Warning**: The contents of a webpage are messy and may not be obvious for the first time. However, if you want to scrape any website, you will have to be patient and look through the contents to extract the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching in the webpage\n",
    "\n",
    "You can programmatically search through a webpage to find the tables that are available on the webpage. You can do that by using **`find_all()`** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_table = page_soup.find_all(\"table\")\n",
    "print(len(states_table))\n",
    "states_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScrapping through Application Programming Interface (API)\n",
    "\n",
    "There are a lot of APIs available for each of the website. You can use these APIs to scrape websites like Twitter, Google Trends, etc. \n",
    "\n",
    "In this section, we will use a simple API provided by NASA, [here](http://open-notify.org/), to retrieve data about the International Space Station (ISS). \n",
    "\n",
    "Some of the content presented here is based on [dataquest](https://www.dataquest.io/blog/python-api-tutorial/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current ISS position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various status codes that you will get when you request a website. [This](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) describes more detailed description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "pd.read_json(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current Number of People In Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "pd.read_json(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Maps API\n",
    "\n",
    "You need to install `googlemaps` package in order to use this. \n",
    "\n",
    "Select `Anaconda Prompt` on your computer and then type `pip install --user googlemaps`. This should install googlemaps package that we can use here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google API key\n",
    "\n",
    "Ideally you would need to create this key from your google API dashboard by logging in with your google accounts. I have provided this key to for a dummy account. It comes with it's own restrictions. You may want to create this for your own accounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The below key `AIzaSyC7sJdwW-skSS0UOR-OFOHeGRNa8TwoM18` might be disabled after the class. You can create your won key using the link [here](https://support.google.com/googleapi/answer/6158862?hl=en). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='AIzaSyDCdQCVKWQNNhERNEmuufTwmhDeDszV1ws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_zip = pd.read_csv(\"./data/uscities_zip.csv\", index_col = ['city', 'state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_object = datetime.strptime('Apr 30 2022  3:00PM', '%b %d %Y %I:%M%p')\n",
    "datetime_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text = gmaps.distance_matrix((cities_zip.loc['Detroit', 'MI']['lat'], \n",
    "                                   cities_zip.loc['Detroit', 'MI']['lng']),\n",
    "                                  \n",
    "                                  (cities_zip.loc['Chicago', 'IL']['lat'], \n",
    "                                   cities_zip.loc['Chicago', 'IL']['lng']), \n",
    "                                    \n",
    "                                  departure_time= datetime_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
